{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, os, datetime, time, re, matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data, remove duplicates, drop NaN rows\n",
    "NY Times, Washington Post, WSJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Read NY Times\n",
    "dfNYT = pd.read_csv('/mnt/data/NYT.txt',encoding='utf-8',sep='|',index_col=0)\n",
    "### Read Washington Post (Need to remove duplicates)\n",
    "dfWP = pd.read_csv('/mnt/data/WashingtonPost.txt',encoding='utf-8',sep='|',index_col=0)\n",
    "### Read Wall Street Journal (Need to remove NaN rows and duplicates)\n",
    "dfWSJ = pd.read_csv('/mnt/data/WSJ.txt',encoding='utf-8',sep='|',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Remove Washington Post duplicates\n",
    "dfWP.drop_duplicates(subset=['content'],keep='first',inplace=True)\n",
    "dfWP.reset_index(drop=True,inplace=True)\n",
    "### Remove Wall Street Journal NaN rows and duplicates\n",
    "dfWSJ.dropna(subset=['Cleaned_text'],inplace=True)\n",
    "dfWSJ.drop_duplicates(subset=['Cleaned_text'],keep='first',inplace=True)\n",
    "dfWSJ.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run text analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AllTexts = dfNYT['new_text']\n",
    "AllTexts = AllTexts.append(dfWP['content'])\n",
    "AllTexts = AllTexts.append(dfWSJ['Cleaned_text']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXTS = list(AllTexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os, codecs\n",
    "scriptpath = '/home/ubuntu/Codes/Text-Analytics-Module-master/Code'\n",
    "sys.path.append(scriptpath)\n",
    "import basic_text_processing_functions as tx\n",
    "from basic_text_processing_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')          # For spacy\n",
    "# import en_core_web_sm           # For windows\n",
    "# nlp = en_core_web_sm.load()     # For windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pathloc = '/mnt/data/TextAnalysis/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pnodes = 16\n",
    "if 1==0:\n",
    "    with codecs.open(pathloc+'default.cfg','w',encoding='utf-8') as f:\n",
    "        f.write(json.dumps({'batch_size':1000,'n_threads':pnodes,'fpathroot':pathloc,'fpathappend':u'','entity_sub':True}))\n",
    "    batch_size,n_threads,fpathroot,fpathappend,entity_sub,numtopics = tx._config_text_analysis_(pathloc+'default.cfg')\n",
    "else:\n",
    "    batch_size,n_threads,fpathroot,fpathappend,entity_sub,numtopics = tx._config_text_analysis_(pathloc+'default.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tx.batch_size,tx.n_threads,tx.fpathroot,tx.fpathappend,tx.entity_sub,tx.numtopics = batch_size,n_threads,fpathroot,fpathappend,entity_sub,numtopics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx.fpathroot, fpathroot, pathloc, fpathappend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx.n_threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Write texts to disk\n",
    "fname = 'rawtext.txt'\n",
    "if 1==0:\n",
    "    with codecs.open(pathloc+fname,'w',encoding='utf-8') as f:\n",
    "        for t in TEXTS:\n",
    "            try:\n",
    "#                 f.write(t.replace('\\n',' ')+u'\\n')\n",
    "                f.write(u' '.join(t.splitlines())+u'\\n')\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 1==0:\n",
    "    tx._write_unigram_(pathloc+fname,unigram_sentences_filepath=fpathroot+fpathappend+'_sent_gram_0.txt',entity_sub=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Phrase Models\n",
    "passes = 2\n",
    "if 1==0:\n",
    "    ngrams = tx._phrase_detection_(fpath=tx.fpathroot+tx.fpathappend,passes=passes,returnmodels=True,threshold=10.)\n",
    "else:\n",
    "    ngrams = list()\n",
    "    gramfiles = os.listdir(tx.fpathroot)\n",
    "    phrasemodels = [tx.fpathroot+g for g in gramfiles if 'phrase_model' in g]\n",
    "    for m in phrasemodels:\n",
    "        ngrams.append(Phrases.load(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regram = 1 # change to 1 to organize bags of words by documents (vs. sentences)\n",
    "if regram==1:\n",
    "    if 1==0:\n",
    "        grammed_texts = tx._phrase_prediction_(pathloc+fname,ngrams,outfpath=fpathroot+fpathappend+'grammed_texts.txt',entity_sub=True)\n",
    "    else:\n",
    "        grammed_texts = tx.fpathroot+'grammed_texts.txt'\n",
    "else:\n",
    "    grammed_texts = tx.fpathroot+'sent_gram_{}.txt'.format(passes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "### Create Dictionary\n",
    "if 1==0:\n",
    "    vocab,gensim_dictionary,cts = tx._make_dict_(grammed_texts,floc=fpathroot+fpathappend+'dict_gram.dict',\n",
    "                                                 topfilter=99,bottomfilter=25,no_filters=False,keep_ent=False,\n",
    "                                                 discard_list={},keep_list={})\n",
    "    print(len(vocab))\n",
    "else:\n",
    "    gensim_dictionary = corpora.Dictionary.load(tx.fpathroot+'dict_gram.dict')\n",
    "    print(len(gensim_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 1==0:\n",
    "    grammed_corpus = tx._serialize_corpus_(grammed_texts,gensim_dictionary,outfpath=fpathroot+fpathappend+'_serialized.mm')\n",
    "else:\n",
    "    grammed_corpus_loc = tx.fpathroot+tx.fpathappend+'_serialized.mm'\n",
    "    grammed_corpus = MmCorpus(grammed_corpus_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Perform LDA\n",
    "numtopics = 5\n",
    "ldafile = 'lda_'+str(numtopics)\n",
    "if 1==0:\n",
    "    lda = tx._lda_(gensim_dictionary,corpus_path=grammed_corpus,numtopics=numtopics,iterations=100) # defaults to 10 topics\n",
    "    lda.save(pathloc+ldafile)\n",
    "else: \n",
    "    lda = LdaMulticore.load(pathloc+ldafile)\n",
    "lda.minimum_probability = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Create Visualization of Topics\n",
    "import pyLDAvis.gensim as ldavis\n",
    "if 1==0:\n",
    "    ldaviz = ldavis.prepare(lda,grammed_corpus,gensim_dictionary)\n",
    "    pyLDAvis.save_html(ldaviz,pathloc+'viz_'+ldafile+'.html')\n",
    "lda.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numrange = np.arange(5,115,5)\n",
    "numtopiclist = [a for a in numrange if a not in [50,70,100]]\n",
    "### Perform LDA\n",
    "for numtopics in numtopiclist:\n",
    "    ldafile = 'lda_'+str(numtopics)\n",
    "    if 1==0:\n",
    "        lda = tx._lda_(gensim_dictionary,corpus_path=grammed_corpus,numtopics=numtopics,iterations=100)\n",
    "        lda.save(pathloc+ldafile)\n",
    "    else: \n",
    "        lda = LdaMulticore.load(pathloc+ldafile)\n",
    "    lda.minimum_probability = 0.0\n",
    "    if 1==0:\n",
    "        ldaviz = ldavis.prepare(lda,grammed_corpus,gensim_dictionary)\n",
    "        pyLDAvis.save_html(ldaviz,pathloc+'viz_'+ldafile+'.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict back topic probabilities and get word counts\n",
    "NY Times, Washington Post, WSJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Predict topics for NY Times\n",
    "topicsNYT = dfNYT[['date','new_text']].rename(columns={'new_text':'content'})\n",
    "topicsNYT.dropna(subset=['content'],inplace=True)\n",
    "topicsNYT.reset_index(drop=True,inplace=True)\n",
    "textNYT = list(topicsNYT['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "topicsNYT['parsedtexts'] = tx._phrase_prediction_inmemory_(textNYT,ngrams)\n",
    "topicsNYT['topic_probs'] = topicsNYT['parsedtexts'].apply(lambda x: dict(lda[gensim_dictionary.doc2bow(x.split())]))\n",
    "for t in range(0,numtopics):\n",
    "    topicsNYT['topic_'+str(t)] = topicsNYT.topic_probs.apply(lambda x: tx._try_iter_(x,t,errval=0.))\n",
    "topicsNYT['word_ct'] = topicsNYT['content'].apply(lambda x: len(unicode(x).split()))\n",
    "for t in range(0,numtopics):\n",
    "    topicsNYT['word_ct_topic_'+str(t)] = topicsNYT.apply(lambda x: x['topic_'+str(t)]*x['word_ct'],axis=1)\n",
    "run_time = (time.time()-start_time)/3600\n",
    "print (\"This takes %s hours to run\" %run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topicsNYT.dropna(subset=['parsedtexts'],inplace=True)\n",
    "# topicsNYT.drop_duplicates(subset='parsedtexts',keep='first',inplace=True)\n",
    "topicsNYT.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topicsNYT.to_csv('/mnt/data/TextAnalysis/topicsNYT.txt',sep='|',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Predict topics for Washington Post\n",
    "topicsWP = dfWP[['date','content']].copy()\n",
    "topicsWP.dropna(subset=['content'],inplace=True)\n",
    "topicsWP.reset_index(drop=True,inplace=True)\n",
    "textWP = list(topicsWP['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "topicsWP['parsedtexts'] = tx._phrase_prediction_inmemory_(textWP,ngrams)\n",
    "topicsWP['topic_probs'] = topicsWP['parsedtexts'].apply(lambda x: dict(lda[gensim_dictionary.doc2bow(x.split())]))\n",
    "for t in range(0,numtopics):\n",
    "    topicsWP['topic_'+str(t)] = topicsWP.topic_probs.apply(lambda x: tx._try_iter_(x,t,errval=0.))\n",
    "topicsWP['word_ct'] = topicsWP['content'].apply(lambda x: len(unicode(x).split()))\n",
    "for t in range(0,numtopics):\n",
    "    topicsWP['word_ct_topic_'+str(t)] = topicsWP.apply(lambda x: x['topic_'+str(t)]*x['word_ct'],axis=1)\n",
    "run_time = (time.time()-start_time)/3600\n",
    "print (\"This takes %s hours to run\" %run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topicsWP.dropna(subset=['parsedtexts'],inplace=True)\n",
    "# topicsWP.drop_duplicates(subset='parsedtexts',keep='first',inplace=True)\n",
    "topicsWP.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topicsWP.to_csv('/mnt/data/TextAnalysis/topicsWP.txt',sep='|',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Predict topics for WSJ\n",
    "topicsWSJ = dfWSJ[['Publication date','Cleaned_text']].rename(columns={'Publication date':'date',\n",
    "                                                                       'Cleaned_text':'content'})\n",
    "topicsWSJ.dropna(subset=['content'],inplace=True)\n",
    "topicsWSJ.reset_index(drop=True,inplace=True)\n",
    "textWSJ = list(topicsWSJ['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "topicsWSJ['parsedtexts'] = tx._phrase_prediction_inmemory_(textWSJ,ngrams)\n",
    "topicsWSJ['topic_probs'] = topicsWSJ['parsedtexts'].apply(lambda x: dict(lda[gensim_dictionary.doc2bow(x.split())]))\n",
    "for t in range(0,numtopics):\n",
    "    topicsWSJ['topic_'+str(t)] = topicsWSJ.topic_probs.apply(lambda x: tx._try_iter_(x,t,errval=0.))\n",
    "topicsWSJ['word_ct'] = topicsWSJ['content'].apply(lambda x: len(unicode(x).split()))\n",
    "for t in range(0,numtopics):\n",
    "    topicsWSJ['word_ct_topic_'+str(t)] = topicsWSJ.apply(lambda x: x['topic_'+str(t)]*x['word_ct'],axis=1)\n",
    "run_time = (time.time()-start_time)/3600\n",
    "print (\"This takes %s hours to run\" %run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topicsWSJ.dropna(subset=['parsedtexts'],inplace=True)\n",
    "# topicsWSJ.drop_duplicates(subset='parsedtexts',keep='first',inplace=True)\n",
    "topicsWSJ.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topicsWSJ.to_csv('/mnt/data/TextAnalysis/topicsWSJ.txt',sep='|',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Plot topic probabilities of NY Times\n",
    "for t in range(0,numtopics):\n",
    "    plt.figure()\n",
    "    np.log(topicsNYT['topic_'+str(t)]).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Plot topic probabilities of Washington Post\n",
    "for t in range(0,numtopics):\n",
    "    plt.figure()\n",
    "    np.log(topicsWP['topic_'+str(t)]).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Plot topic probabilities of WSJ\n",
    "for t in range(0,numtopics):\n",
    "    plt.figure()\n",
    "    np.log(topicsWSJ['topic_'+str(t)]).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate relevance(term w | topic t)\n",
    "NY Times, Washington Post, WSJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionary assigns words to ids\n",
    "# Corpus counts frequencies of words in each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # ldatopictopterms = lda.print_topic(16,topn=30)\n",
    "# ldatopictopterms = lda.show_topic(16,topn=30)\n",
    "# ldatopictopids = lda.get_topic_terms(16,topn=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 50\n",
    "### Find probabilities of top N terms in a topic for all topics = p(w|t)\n",
    "ProbTopTerms = list()\n",
    "ProbTopIDs = list()\n",
    "for t in range(0,numtopics):\n",
    "    ProbTopTerms.append(lda.show_topic(t,topn=N))\n",
    "    ProbTopIDs.append(lda.get_topic_terms(t,topn=N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ProbTopTerms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ProbTopIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gensim_dictionary.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(gensim_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newcorpus = [item for sublist in grammed_corpus for item in sublist]\n",
    "len(newcorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newcorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfcorpus = pd.DataFrame(newcorpus,columns=['termID','count'])\n",
    "dfcorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Count term frequencies in the entire corpus\n",
    "dfcorpusgroup = dfcorpus.groupby(pd.Grouper(key='termID')).sum()\n",
    "dfcorpusgroup.reset_index(level=0,inplace=True)\n",
    "dfcorpusgroup = dfcorpusgroup.sort_values(by=['termID']).reset_index(drop=True)\n",
    "dfcorpusgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topicsNYT = pd.read_csv('/mnt/data/TextAnalysis/topicsNYT.txt',encoding='utf-8',sep='|',index_col=0)\n",
    "topicsWP = pd.read_csv('/mnt/data/TextAnalysis/topicsWP.txt',encoding='utf-8',sep='|',index_col=0)\n",
    "topicsWSJ = pd.read_csv('/mnt/data/TextAnalysis/topicsWSJ.txt',encoding='utf-8',sep='|',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### NY Times, Washington Post, WSJ\n",
    "topicsNYTnew = topicsNYT.iloc[:,:(numtopics+5)].copy()\n",
    "topicsWPnew = topicsWP.iloc[:,:(numtopics+5)].copy()\n",
    "topicsWSJnew = topicsWSJ.iloc[:,:(numtopics+5)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Method 1: use word counts from contents\n",
    "wc1 = topicsNYTnew['word_ct'].sum()\n",
    "wc2 = topicsWPnew['word_ct'].sum()\n",
    "wc3 = topicsWSJnew['word_ct'].sum()\n",
    "wordcount1 = wc1+wc2+wc3\n",
    "wordcount1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfcorpusgroup1 = dfcorpusgroup.copy()\n",
    "dfcorpusgroup1['docfreq'] = dfcorpusgroup1['count']/wordcount1\n",
    "dfcorpusgroup1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Method 2: use word counts from the whole corpus\n",
    "wordcount2 = dfcorpusgroup['count'].sum()\n",
    "wordcount2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfcorpusgroup2 = dfcorpusgroup.copy()\n",
    "dfcorpusgroup2['docfreq'] = dfcorpusgroup2['count']/wordcount2\n",
    "dfcorpusgroup2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate relevance of each term\n",
    "ld = 0.6\n",
    "RelevanceTerms = list()\n",
    "for t,topic in enumerate(ProbTopIDs):\n",
    "    result = list()\n",
    "    for w,term in enumerate(topic):\n",
    "        termID = term[0]\n",
    "        word = ProbTopTerms[t][w][0]\n",
    "        prob = term[1]\n",
    "        docfreq = dfcorpusgroup2['docfreq'][int(termID)]\n",
    "        relevance = ld*np.log(prob)+(1-ld)*np.log(prob/docfreq)\n",
    "        result.append(tuple([termID,word,prob,docfreq,relevance]))\n",
    "    RelevanceTerms.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find most relevant terms in each topic\n",
    "topterms = list()\n",
    "for t,topic in enumerate(RelevanceTerms):\n",
    "    df = pd.DataFrame(topic,columns=['termID','term','prob','docfreq','relevance']).sort_values(by=['relevance'],ascending=False).reset_index(drop=True)\n",
    "    terms = list(df['term'][0:20])\n",
    "    topterms.append(terms)\n",
    "topterms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the Gaussian Mixture Model\n",
    "\n",
    "Links to documentation:\n",
    "\n",
    "http://scikit-learn.org/stable/user_guide.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import mixture\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_components = 2             # Number of mixture components. Defaults to 1 \n",
    "covariance_type = 'full'     # {'full','tied','diag','spherical'}. Defaults to 'full' \n",
    "tol = 0.001                  # Convergence threshold. EM iterations will stop when the lower bound average gain is below this threshold. Defaults to 1e-3=0.001 \n",
    "reg_covar = 1e-06            # Non-negative regularization added to the diagonal of covariance. Allows to assure that the covariance matrices are all positive. Defaults to 1e-6 \n",
    "max_iter = 100               # Number of EM iterations to perform. Defaults to 100 \n",
    "n_init = 1                   # Number of initializations to perform. The best results are kept. Defaults to 1 \n",
    "init_params = 'kmeans'       # {'kmeans','random'}. Method used to initialize the weights, the means and the precisions. Defaults to 'kmeans' \n",
    "weights_init = None          # User-provided initial weights. Defaults to None (weights are initialized using the init_params method) \n",
    "means_init = None            # User-provided initial means. Defaults to None (means are initialized using the init_params method) \n",
    "precisions_init = None       # User-provided initial precisions (inverse of the covariance matrices). Defaults to None (precisions are initialized using the 'init_params' method). The shape depends on 'covariance_type' \n",
    "random_state = None          # Defaults to None. If int, random_state is the seed used by the random number generator. If RandomState instance, random_state is the random number generator. If None, the random number generator is the RandomState instance used by np.random \n",
    "warm_start = False           # Defaults to False. If True, the solution of the last fitting is used as initialization for the next call of fit(). This can speed up convergence when fit is called several time on similar problems. \n",
    "verbose = 0                  # Defaults to 0. Enable verbose output. If 1 then it prints the current initialization and each iteration step. If greater than 1 then it prints also the log probability and the time needed for each step \n",
    "verbose_interval = 10        # Number of iteration done before the next print. Defaults to 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gmm = mixture.GaussianMixture(n_components=n_components,covariance_type=covariance_type)\n",
    "gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict Gaussian Mixture Model for all topics of NY Times\n",
    "TopicArraysNYT = []\n",
    "PredictTopicsNYT = []\n",
    "for t in range(0,numtopics):\n",
    "    TopicArray = np.asarray(topicsNYT['topic_'+str(t)])\n",
    "    X = TopicArray.reshape(-1,1)\n",
    "    gmm = gmm.fit(X)\n",
    "    Z = gmm.predict(X)\n",
    "    if np.bincount(Z)[0]<=np.bincount(Z)[1]:\n",
    "        Y = (Z-1)*(Z-1)\n",
    "    else:\n",
    "        Y = Z\n",
    "    TopicArraysNYT.append(X)\n",
    "    PredictTopicsNYT.append(Y)\n",
    "    plt.figure()\n",
    "    plt.hist(Y)\n",
    "    plt.title('Histogram for NY Times: topic_'+str(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict Gaussian Mixture Model for all topics of Washington Post\n",
    "TopicArraysWP = []\n",
    "PredictTopicsWP = []\n",
    "for t in range(0,numtopics):\n",
    "    TopicArray = np.asarray(topicsWP['topic_'+str(t)])\n",
    "    X = TopicArray.reshape(-1,1)\n",
    "    gmm = gmm.fit(X)\n",
    "    Z = gmm.predict(X)\n",
    "    if np.bincount(Z)[0]<=np.bincount(Z)[1]:\n",
    "        Y = (Z-1)*(Z-1)\n",
    "    else:\n",
    "        Y = Z\n",
    "    TopicArraysWP.append(X)\n",
    "    PredictTopicsWP.append(Y)\n",
    "    plt.figure()\n",
    "    plt.hist(Y)\n",
    "    plt.title('Histogram for Washington Post: topic_'+str(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict Gaussian Mixture Model for all topics of WSJ\n",
    "TopicArraysWSJ = []\n",
    "PredictTopicsWSJ = []\n",
    "for t in range(0,numtopics):\n",
    "    TopicArray = np.asarray(topicsWSJ['topic_'+str(t)])\n",
    "    X = TopicArray.reshape(-1,1)\n",
    "    gmm = gmm.fit(X)\n",
    "    Z = gmm.predict(X)\n",
    "    if np.bincount(Z)[0]<=np.bincount(Z)[1]:\n",
    "        Y = (Z-1)*(Z-1)\n",
    "    else:\n",
    "        Y = Z\n",
    "    TopicArraysWSJ.append(X)\n",
    "    PredictTopicsWSJ.append(Y)\n",
    "    plt.figure()\n",
    "    plt.hist(Y)\n",
    "    plt.title('Histogram for WSJ: topic_'+str(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep relevant topics and remove irrelevant topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topicsNYTbinary = topicsNYT.iloc[:,0:(numtopics+5)].copy()\n",
    "for t in range(0,numtopics):\n",
    "    topicsNYTbinary['binarytopic_'+str(t)] = PredictTopicsNYT[t]\n",
    "topicsNYTbinary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topicsWPbinary = topicsWP.iloc[:,0:(numtopics+5)].copy()\n",
    "for t in range(0,numtopics):\n",
    "    topicsWPbinary['binarytopic_'+str(t)] = PredictTopicsWP[t]\n",
    "topicsWPbinary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topicsWSJbinary = topicsWSJ.iloc[:,0:(numtopics+5)].copy()\n",
    "for t in range(0,numtopics):\n",
    "    topicsWSJbinary['binarytopic_'+str(t)] = PredictTopicsWSJ[t]\n",
    "topicsWSJbinary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topicsNYTbinary = pd.read_csv('/mnt/data/TextAnalysis/topicsNYTbinary.txt',encoding='utf-8',sep='|',index_col=0)\n",
    "topicsWPbinary = pd.read_csv('/mnt/data/TextAnalysis/topicsWPbinary.txt',encoding='utf-8',sep='|',index_col=0)\n",
    "topicsWSJbinary = pd.read_csv('/mnt/data/TextAnalysis/topicsWSJbinary.txt',encoding='utf-8',sep='|',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot topic trends over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/time-series-data-visualization-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dftopicsNYT = topicsNYTbinary.iloc[:,(numtopics+5):].copy()\n",
    "dftopicsNYT['date'] = pd.to_datetime(topicsNYTbinary['date'])\n",
    "dftopicsNYT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in dftopicsNYT.iloc[:,:-1].columns:\n",
    "    df = dftopicsNYT[['date',i]].copy()\n",
    "    dfgroup = df.groupby(pd.Grouper(key='date',freq='M')).mean()\n",
    "    words = u', '.join(topterms[int(i[12:])][:10])\n",
    "    plt.figure()\n",
    "    plt.plot(dfgroup)\n",
    "    plt.axvline(x='2011-03-31',color='r',linestyle='--')\n",
    "    plt.title('NY Times '+i[6:]+' ('+words+') by month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dftopicsWP = topicsWPbinary.iloc[:,(numtopics+5):].copy()\n",
    "dftopicsWP['date'] = pd.to_datetime(topicsWPbinary['date'])\n",
    "dftopicsWP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in dftopicsWP.iloc[:,:-1].columns:\n",
    "    df = dftopicsWP[['date',i]].copy()\n",
    "    dfgroup = df.groupby(pd.Grouper(key='date',freq='M')).mean()\n",
    "    words = u', '.join(topterms[int(i[12:])][:10])\n",
    "    plt.figure()\n",
    "    plt.plot(dfgroup)\n",
    "    plt.axvline(x='2013-06-30',color='r',linestyle='--')\n",
    "    plt.title('Washington Post '+i[6:]+' ('+words+') by month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dftopicsWSJ = topicsWSJbinary.iloc[:,(numtopics+5):].copy()\n",
    "dftopicsWSJ['date'] = pd.to_datetime(topicsWSJbinary['date'])\n",
    "dftopicsWSJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in dftopicsWSJ.iloc[:,:-1].columns:\n",
    "    df = dftopicsWSJ[['date',i]].copy()\n",
    "    dfgroup = df.groupby(pd.Grouper(key='date',freq='M')).mean()\n",
    "    words = u', '.join(topterms[int(i[12:])][:10])\n",
    "    plt.figure()\n",
    "    plt.plot(dfgroup)\n",
    "#     plt.axvline(x='2011-03-31',color='r',linestyle='--')\n",
    "    plt.title('WSJ '+i[6:]+' ('+words+') by month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment - polarity and subjectivity\n",
    "Sentiment() returns polarity (positive ↔ negative) and subjectivity (objective ↔ subjective)\n",
    "\n",
    "Modality() returns the degree of certainty as a value between -1.0 and +1.0, where values > +0.5 represent facts\n",
    "\n",
    "Mood() returns either INDICATIVE, IMPERATIVE, CONDITIONAL or SUBJUNCTIVE for a given parsed sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, os, datetime, time, re, matplotlib\n",
    "import pattern.en as pat\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topicsNYT = pd.read_csv('/mnt/data/TextAnalysis/topicsNYT.txt',encoding='utf-8',sep='|',index_col=0)\n",
    "topicsWP = pd.read_csv('/mnt/data/TextAnalysis/topicsWP.txt',encoding='utf-8',sep='|',index_col=0)\n",
    "topicsWSJ = pd.read_csv('/mnt/data/TextAnalysis/topicsWSJ.txt',encoding='utf-8',sep='|',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topicsNYTbinary = pd.read_csv('/mnt/data/TextAnalysis/topicsNYTbinary.txt',encoding='utf-8',sep='|',index_col=0)\n",
    "topicsWPbinary = pd.read_csv('/mnt/data/TextAnalysis/topicsWPbinary.txt',encoding='utf-8',sep='|',index_col=0)\n",
    "topicsWSJbinary = pd.read_csv('/mnt/data/TextAnalysis/topicsWSJbinary.txt',encoding='utf-8',sep='|',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patternNYT = topicsNYT[['date','content','parsedtexts']].copy()\n",
    "patternWP = topicsWP[['date','content','parsedtexts']].copy()\n",
    "patternWSJ = topicsWSJ[['date','content','parsedtexts']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Start IPython Clusters\n",
    "import ipyparallel as ipp\n",
    "from ipyparallel import Client\n",
    "client = Client() # run on local ipcluster\n",
    "# client = Client('Your security group',\n",
    "#                 sshkey='Your key')\n",
    "lbview = client.load_balanced_view()\n",
    "pnodes = len(client.ids)     # Number of nodes in the starcluster\n",
    "print pnodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@lbview.parallel(block=True)\n",
    "def _pattern_(packet):\n",
    "    import pandas as pd, time, datetime, os\n",
    "    import pattern.en as pat\n",
    "    dates, contents, parsedtexts = zip(*packet)\n",
    "    df = pd.DataFrame()\n",
    "    for i,d in enumerate(dates):\n",
    "        content = contents[i]\n",
    "        parsedtext = parsedtexts[i]\n",
    "        result = []\n",
    "        for index,text in enumerate(content):\n",
    "            ptext = parsedtext[index]\n",
    "            if type(ptext)==float:\n",
    "                sent = (u'N/A',u'N/A')\n",
    "            else:\n",
    "                sent = pat.sentiment(text)\n",
    "            result.append([d,text,ptext,sent[0],sent[1]])\n",
    "        data = pd.DataFrame(result)\n",
    "        df = df.append(data)\n",
    "    df.columns = ['date','content','parsedtexts','polarity','subjectivity']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    n = max(1, n)\n",
    "    return [l[i:i+n] for i in range(0,len(l),n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NY Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates = np.unique(patternNYT['date']).tolist()\n",
    "len(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = [(dt,list(patternNYT.loc[patternNYT.date==dt,'content']),\n",
    "           list(patternNYT.loc[patternNYT.date==dt,'parsedtexts'])) for dt in dates]\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterspernode = 1\n",
    "ndates = chunks(inputs,max(int(len(inputs)/(pnodes*iterspernode)),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'each node gets {} dates'.format(len(ndates[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputNYT = pd.DataFrame()\n",
    "start_time = time.time()\n",
    "outputNYT = outputNYT.append(_pattern_.map(ndates))\n",
    "run_time = (time.time()-start_time)/3600\n",
    "print (\"This takes %s hours to run\" %run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outputNYT = outputNYT.reset_index(drop=True)\n",
    "outputNYT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputNYT.to_csv('/mnt/data/TextAnalysis/patternNYT.txt',sep='|',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Washington Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates = np.unique(patternWP['date']).tolist()\n",
    "len(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = [(dt,list(patternWP.loc[patternWP.date==dt,'content']),\n",
    "           list(patternWP.loc[patternWP.date==dt,'parsedtexts'])) for dt in dates]\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterspernode = 1\n",
    "ndates = chunks(inputs,max(int(len(inputs)/(pnodes*iterspernode)),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'each node gets {} dates'.format(len(ndates[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputWP = pd.DataFrame()\n",
    "start_time = time.time()\n",
    "outputWP = outputWP.append(_pattern_.map(ndates))\n",
    "run_time = (time.time()-start_time)/3600\n",
    "print (\"This takes %s hours to run\" %run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outputWP = outputWP.reset_index(drop=True)\n",
    "outputWP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputWP.to_csv('/mnt/data/TextAnalysis/patternWP.txt',sep='|',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wall Street Journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates = np.unique(patternWSJ['date']).tolist()\n",
    "len(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = [(dt,list(patternWSJ.loc[patternWSJ.date==dt,'content']),\n",
    "           list(patternWSJ.loc[patternWSJ.date==dt,'parsedtexts'])) for dt in dates]\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterspernode = 1\n",
    "ndates = chunks(inputs,max(int(len(inputs)/(pnodes*iterspernode)),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'each node gets {} dates'.format(len(ndates[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputWSJ = pd.DataFrame()\n",
    "start_time = time.time()\n",
    "outputWSJ = outputWSJ.append(_pattern_.map(ndates))\n",
    "run_time = (time.time()-start_time)/3600\n",
    "print (\"This takes %s hours to run\" %run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outputWSJ = outputWSJ.reset_index(drop=True)\n",
    "outputWSJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputWSJ.to_csv('/mnt/data/TextAnalysis/patternWSJ.txt',sep='|',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, os, datetime, time, re, matplotlib\n",
    "import pattern.en as pat\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patternNYT = pd.read_csv('/mnt/data/TextAnalysis/patternNYT.txt',encoding='utf-8',sep='|',index_col=0,chunksize=10000)\n",
    "patternNYT = pd.concat(patternNYT)\n",
    "patternWP = pd.read_csv('/mnt/data/TextAnalysis/patternWP.txt',encoding='utf-8',sep='|',index_col=0)\n",
    "patternWSJ = pd.read_csv('/mnt/data/TextAnalysis/patternWSJ.txt',encoding='utf-8',sep='|',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfNYT = patternNYT[['date','polarity']].copy()\n",
    "dfNYT['date'] = pd.to_datetime(dfNYT['date'])\n",
    "dfWP = patternWP[['date','polarity']].copy()\n",
    "dfWP['date'] = pd.to_datetime(dfWP['date'])\n",
    "dfWSJ = patternWSJ[['date','polarity']].copy()\n",
    "dfWSJ['date'] = pd.to_datetime(dfWSJ['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfNYT = outputNYT[['date','polarity']].copy()\n",
    "dfNYT['date'] = pd.to_datetime(dfNYT['date'])\n",
    "dfWP = outputWP[['date','polarity']].copy()\n",
    "dfWP['date'] = pd.to_datetime(dfWP['date'])\n",
    "dfWSJ = outputWSJ[['date','polarity']].copy()\n",
    "dfWSJ['date'] = pd.to_datetime(dfWSJ['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfgroupNYT = dfNYT.groupby(pd.Grouper(key='date',freq='M')).mean()\n",
    "dfgroupWP = dfWP.groupby(pd.Grouper(key='date',freq='M')).mean()\n",
    "dfgroupWSJ = dfWSJ.groupby(pd.Grouper(key='date',freq='M')).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(dfgroupNYT)\n",
    "plt.axvline(x='2011-03-31',color='r',linestyle='--')\n",
    "plt.title('Polarity of NY Times by month',fontsize=20)\n",
    "plt.xlabel('Year', fontsize=16)\n",
    "plt.ylabel('Polarity', fontsize=16)\n",
    "plt.savefig('/mnt/data/TextAnalysis-Figures/PolaritybyMonthNYT.png',bbox_inches='tight')\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(dfgroupWP)\n",
    "plt.axvline(x='2013-06-30',color='r',linestyle='--')\n",
    "plt.title('Polarity of Washington Post by month',fontsize=20)\n",
    "plt.xlabel('Year', fontsize=16)\n",
    "plt.ylabel('Polarity', fontsize=16)\n",
    "plt.savefig('/mnt/data/TextAnalysis-Figures/PolaritybyMonthWP.png',bbox_inches='tight')\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(dfgroupWSJ)\n",
    "plt.title('Polarity of Wall Street Journal by month',fontsize=20)\n",
    "plt.xlabel('Year', fontsize=16)\n",
    "plt.ylabel('Polarity', fontsize=16)\n",
    "plt.savefig('/mnt/data/TextAnalysis-Figures/PolaritybyMonthWSJ.png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment of titles vs sentiment of content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, os, datetime, time, re, matplotlib\n",
    "import pattern.en as pat\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Read NY Times\n",
    "dfNYT = pd.read_csv('/mnt/data/TextAnalysis/NYT.txt',encoding='utf-8',sep='|',index_col=0,chunksize=10000)\n",
    "dfNYT = pd.concat(dfNYT)\n",
    "### Read Wall Street Journal (Need to remove NaN rows and duplicates)\n",
    "dfWSJ = pd.read_csv('/mnt/data/TextAnalysis/WSJ.txt',encoding='utf-8',sep='|',index_col=0)\n",
    "### Remove Wall Street Journal NaN rows and duplicates\n",
    "dfWSJ.dropna(subset=['Cleaned_text'],inplace=True)\n",
    "dfWSJ.drop_duplicates(subset=['Cleaned_text'],keep='first',inplace=True)\n",
    "dfWSJ.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Read pattern files\n",
    "patternNYT = pd.read_csv('/mnt/data/TextAnalysis/patternNYT.txt',encoding='utf-8',sep='|',index_col=0,chunksize=10000)\n",
    "patternNYT = pd.concat(patternNYT)\n",
    "patternWSJ = pd.read_csv('/mnt/data/TextAnalysis/patternWSJ.txt',encoding='utf-8',sep='|',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataNYT = dfNYT[['date','title','content','new_text']].rename(columns={'new_text':'cleaned_text'})\n",
    "dataNYT['parsedtexts'] = patternNYT['parsedtexts']\n",
    "dataNYT['polarity'] = patternNYT['polarity']\n",
    "dataNYT['subjectivity'] = patternNYT['subjectivity']\n",
    "dataNYT[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataWSJ = dfWSJ[['Publication date','Title','Full text','Cleaned_text']].rename(columns={'Publication date':'date',\n",
    "                                                                                         'Title':'title',\n",
    "                                                                                         'Full text':'content',\n",
    "                                                                                         'Cleaned_text':'cleaned_text'})\n",
    "dataWSJ['parsedtexts'] = patternWSJ['parsedtexts']\n",
    "dataWSJ['polarity'] = patternWSJ['polarity']\n",
    "dataWSJ['subjectivity'] = patternWSJ['subjectivity']\n",
    "dataWSJ[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Start IPython Clusters\n",
    "import ipyparallel as ipp\n",
    "from ipyparallel import Client\n",
    "client = Client() # run on local ipcluster\n",
    "# client = Client('Your security group',\n",
    "#                 sshkey='Your key')\n",
    "lbview = client.load_balanced_view()\n",
    "pnodes = len(client.ids)     # Number of nodes in the starcluster\n",
    "print pnodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@lbview.parallel(block=True)\n",
    "def _pattern_title_(packet):\n",
    "    import pandas as pd, numpy as np, time, datetime, os\n",
    "    import pattern.en as pat\n",
    "    dates, titles, contents, cleaned_texts, parsedtexts, polarities, subjectivities = zip(*packet)\n",
    "    df = pd.DataFrame()\n",
    "    for i,d in enumerate(dates):\n",
    "        title = titles[i]\n",
    "        content = contents[i]\n",
    "        cleaned_text = cleaned_texts[i]\n",
    "        parsedtext = parsedtexts[i]\n",
    "        polarity = polarities[i]\n",
    "        subjectivity = subjectivities[i]\n",
    "        result = []\n",
    "        for ind,val in enumerate(title):\n",
    "            sent = pat.sentiment(val)\n",
    "            result.append([d,val,content[ind],cleaned_text[ind],parsedtext[ind],polarity[ind],np.float64(sent[0]),\n",
    "                           subjectivity[ind],np.float64(sent[1])])\n",
    "        data = pd.DataFrame(result)\n",
    "        df = df.append(data)\n",
    "    df.columns = ['date','title','content','cleanedtext','parsedtext','contentpolarity','titlepolarity',\n",
    "                  'contentsubjectivity','titlesubjectivity']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    n = max(1, n)\n",
    "    return [l[i:i+n] for i in range(0,len(l),n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates = np.unique(dataNYT['date']).tolist()\n",
    "len(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = []\n",
    "for dt in dates:\n",
    "    data = dataNYT.loc[dataNYT.date==dt]\n",
    "    inputs.append((dt,list(data['title']),list(data['content']),list(data['cleaned_text']),list(data['parsedtexts']),\n",
    "                   list(data['polarity']),list(data['subjectivity'])))\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterspernode = 1\n",
    "ndates = chunks(inputs,max(int(len(inputs)/(pnodes*iterspernode)),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'each node gets {} dates'.format(len(ndates[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputNYT = pd.DataFrame()\n",
    "start_time = time.time()\n",
    "outputNYT = outputNYT.append(_pattern_title_.map(ndates))\n",
    "run_time = (time.time()-start_time)/60\n",
    "print (\"This takes %s minutes to run\" %run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputNYT = outputNYT.reset_index(drop=True)\n",
    "outputNYT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates = np.unique(dataWSJ['date']).tolist()\n",
    "len(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = []\n",
    "for dt in dates:\n",
    "    data = dataWSJ.loc[dataWSJ.date==dt]\n",
    "    inputs.append((dt,list(data['title']),list(data['content']),list(data['cleaned_text']),list(data['parsedtexts']),\n",
    "                   list(data['polarity']),list(data['subjectivity'])))\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterspernode = 1\n",
    "ndates = chunks(inputs,max(int(len(inputs)/(pnodes*iterspernode)),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'each node gets {} dates'.format(len(ndates[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputWSJ = pd.DataFrame()\n",
    "start_time = time.time()\n",
    "outputWSJ = outputWSJ.append(_pattern_title_.map(ndates))\n",
    "run_time = (time.time()-start_time)/60\n",
    "print (\"This takes %s minutes to run\" %run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputWSJ = outputWSJ.reset_index(drop=True)\n",
    "outputWSJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot sentiment of title and content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfNYT = outputNYT[['date','contentpolarity','titlepolarity']].copy()\n",
    "dfNYT['date'] = pd.to_datetime(dfNYT['date'])\n",
    "dfWSJ = outputWSJ[['date','contentpolarity','titlepolarity']].copy()\n",
    "dfWSJ['date'] = pd.to_datetime(dfWSJ['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfgroupNYT = dfNYT.groupby(pd.Grouper(key='date',freq='M')).mean()\n",
    "dfgroupWSJ = dfWSJ.groupby(pd.Grouper(key='date',freq='M')).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(dfgroupNYT)\n",
    "plt.axvline(x='2011-03-31',color='r',linestyle='--')\n",
    "plt.legend(('Content Polarity','Title Polarity'),loc=0,fontsize='16')\n",
    "plt.title('Polarity of NY Times title and content by month',fontsize=20)\n",
    "plt.xlabel('Year', fontsize=16)\n",
    "plt.ylabel('Polarity', fontsize=16)\n",
    "plt.savefig('/mnt/data/TextAnalysis-Figures/PolarityTitleContentNYT.png',bbox_inches='tight')\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(dfgroupWSJ)\n",
    "plt.legend(('Content Polarity','Title Polarity'),loc=0,fontsize='16')\n",
    "plt.title('Polarity of Wall Street Journal title and content by month',fontsize=20)\n",
    "plt.xlabel('Year', fontsize=16)\n",
    "plt.ylabel('Polarity', fontsize=16)\n",
    "plt.savefig('/mnt/data/TextAnalysis-Figures/PolarityTitleContentWSJ.png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfNYT = outputNYT[['date','contentsubjectivity','titlesubjectivity']].copy()\n",
    "dfNYT['date'] = pd.to_datetime(dfNYT['date'])\n",
    "dfWSJ = outputWSJ[['date','contentsubjectivity','titlesubjectivity']].copy()\n",
    "dfWSJ['date'] = pd.to_datetime(dfWSJ['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfgroupNYT = dfNYT.groupby(pd.Grouper(key='date',freq='M')).mean()\n",
    "dfgroupWSJ = dfWSJ.groupby(pd.Grouper(key='date',freq='M')).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(dfgroupNYT)\n",
    "plt.axvline(x='2011-03-31',color='r',linestyle='--')\n",
    "plt.legend(('Content Polarity','Title Polarity'),loc=0,fontsize='16')\n",
    "plt.title('Subjectivity of NY Times title and content by month',fontsize=20)\n",
    "plt.xlabel('Year', fontsize=16)\n",
    "plt.ylabel('Polarity', fontsize=16)\n",
    "plt.savefig('/mnt/data/TextAnalysis-Figures/SubjectivityTitleContentNYT.png',bbox_inches='tight')\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(dfgroupWSJ)\n",
    "plt.legend(('Content Polarity','Title Polarity'),loc=0,fontsize='16')\n",
    "plt.title('Subjectivity of Wall Street Journal title and content by month',fontsize=20)\n",
    "plt.xlabel('Year', fontsize=16)\n",
    "plt.ylabel('Polarity', fontsize=16)\n",
    "plt.savefig('/mnt/data/TextAnalysis-Figures/SubjectivityTitleContentWSJ.png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfNYT = outputNYT[['date','contentpolarity','titlepolarity']].copy()\n",
    "dfNYT['date'] = pd.to_datetime(dfNYT['date'])\n",
    "dfWSJ = outputWSJ[['date','contentpolarity','titlepolarity']].copy()\n",
    "dfWSJ['date'] = pd.to_datetime(dfWSJ['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "poldifNYT = pd.DataFrame()\n",
    "poldifNYT['date'] = dfNYT['date']\n",
    "poldifNYT['polaritydifference'] = dfNYT['contentpolarity']-dfNYT['titlepolarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "poldifgroupNYT = poldifNYT.groupby(pd.Grouper(key='date',freq='M')).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure\n",
    "# plt.figure(figsize=(20,10))\n",
    "plt.plot(poldifgroupNYT)\n",
    "plt.axvline(x='2011-03-31',color='r',linestyle='--')\n",
    "plt.title('Polarity difference of NY Times title and content by month',fontsize=20)\n",
    "plt.xlabel('Year', fontsize=16)\n",
    "plt.ylabel('Polarity', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poldifWSJ = pd.DataFrame()\n",
    "poldifWSJ['date'] = dfWSJ['date']\n",
    "poldifWSJ['polaritydifference'] = dfWSJ['contentpolarity']-dfWSJ['titlepolarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "poldifgroupWSJ = poldifWSJ.groupby(pd.Grouper(key='date',freq='M')).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure\n",
    "# plt.figure(figsize=(20,10))\n",
    "plt.plot(poldifgroupWSJ)\n",
    "plt.title('Polarity difference of WSJ title and content by month',fontsize=20)\n",
    "plt.xlabel('Year', fontsize=16)\n",
    "plt.ylabel('Polarity', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfNYT = outputNYT[['date','contentsubjectivity','titlesubjectivity']].copy()\n",
    "dfNYT['date'] = pd.to_datetime(dfNYT['date'])\n",
    "dfWSJ = outputWSJ[['date','contentsubjectivity','titlesubjectivity']].copy()\n",
    "dfWSJ['date'] = pd.to_datetime(dfWSJ['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subdifNYT = pd.DataFrame()\n",
    "subdifNYT['date'] = dfNYT['date']\n",
    "subdifNYT['subjectivitydifference'] = dfNYT['contentsubjectivity']-dfNYT['titlesubjectivity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subdifgroupNYT = subdifNYT.groupby(pd.Grouper(key='date',freq='M')).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure\n",
    "# plt.figure(figsize=(20,10))\n",
    "plt.plot(subdifgroupNYT)\n",
    "plt.axvline(x='2011-03-31',color='r',linestyle='--')\n",
    "plt.title('Subjectivity difference of NY Times title and content by month',fontsize=20)\n",
    "plt.xlabel('Year', fontsize=16)\n",
    "plt.ylabel('Subjectivity', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subdifWSJ = pd.DataFrame()\n",
    "subdifWSJ['date'] = dfWSJ['date']\n",
    "subdifWSJ['subjectivitydifference'] = dfWSJ['contentsubjectivity']-dfWSJ['titlesubjectivity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subdifgroupWSJ = subdifWSJ.groupby(pd.Grouper(key='date',freq='M')).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure\n",
    "# plt.figure(figsize=(20,10))\n",
    "plt.plot(subdifgroupWSJ)\n",
    "plt.title('Subjectivity difference of WSJ title and content by month',fontsize=20)\n",
    "plt.xlabel('Year', fontsize=16)\n",
    "plt.ylabel('Subjectivity', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match labels of other newspapers with labels of NYT using LDA topics\n",
    "Method 1: Support Vector Machines (Scikit-learn package)\n",
    "\n",
    "Method 2: Neural Network (TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, os, datetime, time, re, matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Read NY Times\n",
    "dfNYT = pd.read_csv('/mnt/data/TextAnalysis/NYT.txt',encoding='utf-8',sep='|',index_col=0,chunksize=10000)\n",
    "dfNYT = pd.concat(dfNYT)\n",
    "### Read Wall Street Journal\n",
    "dfWSJ = pd.read_csv('/mnt/data/TextAnalysis/WSJ.txt',encoding='utf-8',sep='|',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Remove Wall Street Journal NaN rows and duplicates\n",
    "dfWSJ.dropna(subset=['Cleaned_text'],inplace=True)\n",
    "dfWSJ.drop_duplicates(subset=['Cleaned_text'],keep='first',inplace=True)\n",
    "dfWSJ.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os, codecs\n",
    "scriptpath = '/home/ubuntu/Codes/Text-Analytics-Module-master/Code'\n",
    "sys.path.append(scriptpath)\n",
    "import basic_text_processing_functions as tx\n",
    "from basic_text_processing_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pathloc = '/mnt/data/TextAnalysis/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pnodes = 16\n",
    "if 1==1:\n",
    "    with codecs.open(pathloc+'default.cfg','w',encoding='utf-8') as f:\n",
    "        f.write(json.dumps({'batch_size':1000,'n_threads':pnodes,'fpathroot':pathloc,'fpathappend':u'','entity_sub':True}))\n",
    "    batch_size,n_threads,fpathroot,fpathappend,entity_sub,numtopics = tx._config_text_analysis_(pathloc+'default.cfg')\n",
    "else:\n",
    "    batch_size,n_threads,fpathroot,fpathappend,entity_sub,numtopics = tx._config_text_analysis_(pathloc+'default.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tx.batch_size,tx.n_threads,tx.fpathroot,tx.fpathappend,tx.entity_sub,tx.numtopics = batch_size,n_threads,fpathroot,fpathappend,entity_sub,numtopics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx.fpathroot, fpathroot, pathloc, fpathappend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx.n_threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "### Create Dictionary\n",
    "if 1==0:\n",
    "    vocab,gensim_dictionary,cts = tx._make_dict_(grammed_texts,floc=fpathroot+fpathappend+'dict_gram.dict',\n",
    "                                                 topfilter=99,bottomfilter=25,no_filters=False,keep_ent=False,\n",
    "                                                 discard_list=discard_list,keep_list={})\n",
    "    print(len(vocab))\n",
    "else:\n",
    "    gensim_dictionary = corpora.Dictionary.load(tx.fpathroot+'dict_gram.dict')\n",
    "    print(len(gensim_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 1==0:\n",
    "    grammed_corpus = tx._serialize_corpus_(grammed_texts,gensim_dictionary,outfpath=fpathroot+fpathappend+'_serialized.mm')\n",
    "else:\n",
    "    grammed_corpus_loc = tx.fpathroot+tx.fpathappend+'_serialized.mm'\n",
    "    grammed_corpus = MmCorpus(grammed_corpus_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Perform LDA\n",
    "numtopics = 100\n",
    "ldafile = 'lda_'+str(numtopics)\n",
    "if 1==0:\n",
    "    lda = tx._lda_(gensim_dictionary,corpus_path=grammed_corpus,numtopics=numtopics,iterations=100) # defaults to 10 topics\n",
    "    lda.save(pathloc+ldafile)\n",
    "else: \n",
    "    lda = LdaMulticore.load(pathloc+ldafile)\n",
    "lda.minimum_probability = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Create Visualization of Topics\n",
    "import pyLDAvis.gensim as ldavis\n",
    "if 1==0:\n",
    "    ldaviz = ldavis.prepare(lda,grammed_corpus,gensim_dictionary)\n",
    "    pyLDAvis.save_html(ldaviz,pathloc+'viz_'+ldafile+'.html')\n",
    "lda.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Support vector machines vs logit vs regularized least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topicsNYTbinary = pd.read_csv('/mnt/data/TextAnalysis/topicsNYTbinary.txt',encoding='utf-8',sep='|',index_col=0,chunksize=10000)\n",
    "topicsNYTbinary = pd.concat(topicsNYTbinary)\n",
    "topicsWSJbinary = pd.read_csv('/mnt/data/TextAnalysis/topicsWSJbinary.txt',encoding='utf-8',sep='|',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataNYT = dfNYT.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Create label_1 from subsection_name, subsection_name, and news_desk\n",
    "NYT_Label_1 = []\n",
    "document_type = list(dataNYT['document_type'])\n",
    "type_of_material = list(dataNYT['type_of_material'])\n",
    "news_desk = list(dataNYT['news_desk'])\n",
    "section_name = list(dataNYT['section_name'])\n",
    "subsection_name = list(dataNYT['subsection_name'])\n",
    "dataNYTsub = dataNYT[pd.isnull(dataNYT['subsection_name'])]\n",
    "sectionlist = sorted(dataNYTsub['section_name'].value_counts().index.tolist())\n",
    "subsectionlist = sorted(dataNYT['subsection_name'].value_counts().index.tolist())\n",
    "overlaplist = [a for a in sectionlist if a in subsectionlist]\n",
    "differentlist = [a for a in sectionlist if a not in subsectionlist]\n",
    "for ind,val in enumerate(subsection_name):\n",
    "    if type(val)==float:\n",
    "        if type(section_name[ind])==float:\n",
    "            if type(news_desk[ind])==float:\n",
    "                item = val\n",
    "            elif news_desk[ind] in [u'Business',u'Culture',u'Escapes',u'Foreign',u'Great Homes and Destinations',\n",
    "                                    u'OpEd',u'Science',u'Sunday Review',u'Styles',u'Travel',u'TStyle',u'U.S.',\n",
    "                                    u'Your Money']:\n",
    "                item = news_desk[ind]\n",
    "            elif news_desk[ind].startswith(u'Crosswords'):\n",
    "                item = u'Crosswords & Games'\n",
    "            elif news_desk[ind]==u'N.Y. / Region':\n",
    "                item = u'New York and Region'\n",
    "            elif news_desk[ind]==u'U.S. / 9/11 Anniversary':\n",
    "                item = u'9/11 Anniversary'\n",
    "            elif news_desk[ind]==u'Your Money / Mortgages':\n",
    "                item = u'Mortgages'\n",
    "            else:\n",
    "                item = val\n",
    "        elif section_name[ind] in overlaplist:\n",
    "            item = section_name[ind]\n",
    "            \n",
    "        elif section_name[ind] in differentlist:\n",
    "            if section_name[ind].startswith(u'Admin'):\n",
    "                if document_type[ind]==u'article':\n",
    "                    item = u'Corrections'\n",
    "                else:\n",
    "                    item = u'Multimedia'\n",
    "            if section_name[ind].startswith(u'Arts'):\n",
    "                item = u'Arts'\n",
    "            elif section_name[ind].startswith(u'Auto'):\n",
    "                item = u'Automobiles'\n",
    "            elif section_name[ind].startswith(u'Books'):\n",
    "                item = u'Books'\n",
    "            elif section_name[ind].startswith(u'Business'):\n",
    "                item = u'Business'\n",
    "            elif section_name[ind].startswith(u'Crosswords'):\n",
    "                item = u'Crosswords & Games'\n",
    "            elif section_name[ind].startswith(u'Dining'):\n",
    "                item = u'Dining & Wine'\n",
    "            elif section_name[ind] in (u'Editorials',u'Public Editor'):\n",
    "                item = u'Editorials'\n",
    "            elif section_name[ind].startswith(u'Education'):\n",
    "                item = u'Education'\n",
    "            elif section_name[ind].startswith(u'Front Page'):\n",
    "                text = section_name[ind][section_name[ind].find(';')+2:]\n",
    "                if text.find(';')==-1:\n",
    "                    item = text\n",
    "                else:\n",
    "                    item = text[:text.find(';')]\n",
    "            elif section_name[ind].startswith(u'Great Homes'):\n",
    "                item = u'Great Homes and Destinations'\n",
    "            elif section_name[ind].startswith(u'Health'):\n",
    "                item = u'Health'\n",
    "            elif section_name[ind] in [u'Home & Garden',u'Home and Garden; Style']:\n",
    "                item = u'Home & Garden'\n",
    "            elif section_name[ind] in [u'Learning',u'The Learning Network']:\n",
    "                item = u'Learning'\n",
    "            elif section_name[ind].startswith(u'Magazine'):\n",
    "                item = u'Magazine'\n",
    "            elif section_name[ind].startswith(u'Movies'):\n",
    "                item = u'Movies'\n",
    "            elif section_name[ind].startswith(u'Multimedia'):\n",
    "                item = u'Multimedia'\n",
    "            elif section_name[ind] in [u'N.Y. / Region',u'New York',u'New York and Region']:\n",
    "                item = u'New York and Region'\n",
    "            elif section_name[ind].startswith(u'Obituaries'):\n",
    "                item = u'Obituaries'\n",
    "            elif section_name[ind].startswith(u'Opinion'):\n",
    "                item = u'Opinion'\n",
    "            elif section_name[ind].startswith(u'Science'):\n",
    "                item = u'Science'\n",
    "            elif section_name[ind].startswith(u'Sports'):\n",
    "                item = u'Sports'\n",
    "            elif section_name[ind].startswith(u'Style') or section_name[ind] in [u'Booming',u'National',u'T:Style']:\n",
    "                item = u'Style'\n",
    "            elif section_name[ind].startswith(u'Technology'):\n",
    "                item = u'Technology'\n",
    "            elif section_name[ind].startswith(u'Theater'):\n",
    "                item = u'Theater'\n",
    "            elif section_name[ind].startswith(u'Today\\u2019s Paper'):\n",
    "                item = u\"Today's Paper\"\n",
    "            elif section_name[ind].startswith(u'Travel'):\n",
    "                item = u'Travel'\n",
    "            elif section_name[ind] in [u'U.S.',u'U.S.; Obituaries',u'U.S.; Washington']:\n",
    "                item = u'U.S.'\n",
    "            elif section_name[ind].startswith(u'Washington'):\n",
    "                item = u'Washington'\n",
    "            elif section_name[ind].startswith(u'Week'):\n",
    "                item = u'Week in Review'\n",
    "            elif section_name[ind].startswith(u'World'):\n",
    "                item = u'World'\n",
    "            else:\n",
    "                item = section_name[ind]\n",
    "        else:\n",
    "            item = val\n",
    "    elif val==u'false':\n",
    "        if news_desk[ind]==u'National':\n",
    "            item = u'Politics'\n",
    "        elif type(news_desk[ind])!=float:\n",
    "            item = news_desk[ind]\n",
    "        else:\n",
    "            item = val\n",
    "    else:\n",
    "        item = val\n",
    "    if item==u'Art':\n",
    "        label = u'Arts'\n",
    "    elif item==u'Dealbook':\n",
    "        label = u'DealBook'\n",
    "    elif item==u'Dining &amp; Wine':\n",
    "        label = u'Dining & Wine'\n",
    "    elif item==u'Fashion &amp; Style':\n",
    "        label = u'Fashion & Style'\n",
    "    elif item==u'Media &amp; Advertising':\n",
    "        label = u'Media & Advertising'\n",
    "    elif item==u'Men\\u2019s Style':\n",
    "        label = u\"Men's Style\"\n",
    "    elif item==u'Money &amp; Policy':\n",
    "        label = u'Money & Policy'\n",
    "    elif item==u'Weddings / Celebrations':\n",
    "        label = u'Weddings/Celebrations'\n",
    "    else:\n",
    "        label = item\n",
    "    NYT_Label_1.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataNYT['label_1'] = NYT_Label_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Group label_1 to get label_2\n",
    "NYT_Label_2 = []\n",
    "for ind,val in enumerate(NYT_Label_1):\n",
    "    if val in [u\"401(k)'s and Similar Plans\",u'Estate Planning',u\"Individual Retirement Accounts (IRA's)\",\n",
    "               u'Life and Disability Insurance',u'Retirement']:\n",
    "        NYT_Label_2.append(u'Retirement Plans')\n",
    "    elif val in [u'Annuities',u'Asset Allocation',u'Brokerage and Bank Accounts',u'Credit Scores',\n",
    "                 u'Credit and Debit Cards',u'Financial Planners',u'Household Budgeting',u'Mutual Funds',\n",
    "                 u'Mutual Funds and ETFs',u'Paying for College',u'Stocks and Bonds',u'Your Money']:\n",
    "        NYT_Label_2.append(u'Finance, Banking, and Investment')\n",
    "    elif val in [u'Arts',u'Art & Design',u'Dance',u'Design',u'Design & Interiors',u'International Arts',u'Theater',\n",
    "                 u'Theater Reviews',u'Tony Awards']:\n",
    "        NYT_Label_2.append(u'Art and Design')\n",
    "    elif val in [u'Africa',u'Americas',u'Asia Pacific',u'Australia',u'Canada',u'Europe',u'Foreign',\n",
    "                 u'International Home',u'International Opinion',u'Middle East',u'Reach of War',u'What in the World',\n",
    "                 u'World']:\n",
    "        NYT_Label_2.append(u'World News')\n",
    "    elif val in [u'Auto Insurance',u'Auto Loans',u'Automobiles',u'Collectible Cars',u'New Cars',u'Wheels']:\n",
    "        NYT_Label_2.append(u'Automobiles')\n",
    "    elif val in [u'Auto Racing',u'Baseball',u'College Basketball',u'College Football',u'Cricket',u'Cycling',\n",
    "                 u'Global Sports',u'Golf',u'Hockey',u'Horse Racing',u'More Sports',u'N.B.A.',u'Olympics',\n",
    "                 u'Olympics 2010',u'Pro Basketball',u'Pro Football',u'Rugby',u'Sailing',u'Skiing',u'Soccer',u'Sports',\n",
    "                 u'Tennis',u'World Cup',u'International Sports']:\n",
    "        NYT_Label_2.append(u'Sports')\n",
    "    elif val in [u'Awards Season',u'DVD',u'Entertainment',u'Movies',u'Music',u'Television']:\n",
    "        NYT_Label_2.append(u'Music, Movies, and Entertainment')\n",
    "    elif val in [u'Beauty',u'Couture Runway',u'Culture',u'Fashion & Beauty',u'Fashion & Style',u'Fashion Shows',\n",
    "                 u'International Style',u\"Men's Fashion\",u\"Men's Style\",u'Style',u'T Magazine',u'Trends',u'Weddings',\n",
    "                 u'Weddings/Celebrations',u\"Women's Fashion\"]:\n",
    "        NYT_Label_2.append(u'Beauty, Fashion, and Style')\n",
    "    elif val in [u'Books',u'Book Review',u'First Chapters',u'Sunday Book Review']:\n",
    "        NYT_Label_2.append(u'Books & Book Review')\n",
    "    elif val in [u'Bridge',u'Chess']:\n",
    "        NYT_Label_2.append(u'Crosswords & Games')\n",
    "    elif val in [u'Business',u'Business Computing',u'Companies',u'DealBook',u'Entrepreneurship',u'Identify Theft',\n",
    "                 u'International Business',u'Internet',u'Job Market',u'Media',u'Media & Advertising',u'Personal Tech',\n",
    "                 u'Personal Tech Extra',u'Small Business',u'Small Business Email',u'Start-Ups',\n",
    "                 u\"Stuart Elliott's In Advertising\",u'Technology']:\n",
    "        NYT_Label_2.append(u'Business and Technology')\n",
    "    elif val in [u'Campaign Stops',u'Columnists',u'Contributors',u'none',u'Opinion Today Email',u'Politics']:\n",
    "        NYT_Label_2.append(u'Politics')\n",
    "    elif val in [u'Commercial',u'Commercial Real Estate',u'Communities',u'Home Insurance',u'Key Magazine',u'Mortgages',\n",
    "                 u'Real Estate',u'manhattan']:\n",
    "        NYT_Label_2.append(u'Real Estate')\n",
    "    elif val in [u'Connecticut',u'Long Island',u'New Jersey',u'New York and Region',u'The City',u'Westchester']:\n",
    "        NYT_Label_2.append(u'Regional News')\n",
    "    elif val in [u'Dining & Wine',u'Eat',u'Food',u'Wine, Beer & Cocktails']:\n",
    "        NYT_Label_2.append(u'Food, Cooking, and Dining')\n",
    "    elif val in [u'Economy']:\n",
    "        NYT_Label_2.append(u'Economy')\n",
    "    elif val in [u'Education',u'Education Life',u'Lesson Plans',u'Student Loans']:\n",
    "        NYT_Label_2.append(u'Education')\n",
    "    elif val in [u'Election 2016',u'Elections']:\n",
    "        NYT_Label_2.append(u'Elections')\n",
    "    elif val in [u'Energy & Environment ',u'Environment',u'Science', u'Space & Cosmos']:\n",
    "        NYT_Label_2.append(u'Science and Environment')\n",
    "    elif val in [u'Escapes',u'Travel']:\n",
    "        NYT_Label_2.append(u'Travel')\n",
    "    elif val in [u'Family',u'Fitness & Nutrition',u'Health',u'Health Insurance',u'Live',u'Mind',u'Money & Policy',\n",
    "                 u'Move',u'Research',u'Views']:\n",
    "        NYT_Label_2.append(u'Health and Wellness')\n",
    "    elif val in [u'Great Homes and Destinations',u'Great Homes and Destinations Multimedia']:\n",
    "        NYT_Label_2.append(u'Great Homes and Destinations')\n",
    "    else:\n",
    "        NYT_Label_2.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataNYT['label_2'] = NYT_Label_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Create new dataframe from original dataframe and topic probabilities\n",
    "columns1 = ['date','title','headline','author','content','new_text','document_type','type_of_material','news_desk',\n",
    "            'keywords','section_name','subsection_name','print_page','word_count','url','label_1','label_2']\n",
    "columns2 = [topicsNYTbinary.columns.tolist()[2:4],topicsNYTbinary.columns.tolist()[104:105],\n",
    "            topicsNYTbinary.columns.tolist()[4:104],topicsNYTbinary.columns.tolist()[105:]]\n",
    "columns2 = [a for sublist in columns2 for a in sublist]\n",
    "datNYT1 = dataNYT[columns1].copy()\n",
    "datNYT2 = topicsNYTbinary[columns2].copy()\n",
    "datNYT = pd.concat([datNYT1,datNYT2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Get dataframe that has labels\n",
    "labeledNYT = datNYT[pd.notnull(datNYT['label_2'])]\n",
    "labeledNYT.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Predict labels from topics using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import svm, preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, Normalizer, MinMaxScaler\n",
    "from sklearn.preprocessing import QuantileTransformer, PowerTransformer\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Get topics\n",
    "X = labeledNYT.iloc[:,20:120]\n",
    "X = X.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Try different data scalers\n",
    "Xstandard = StandardScaler().fit_transform(X)\n",
    "Xrobust = RobustScaler().fit_transform(X)\n",
    "Xnormalize = Normalizer().fit_transform(X)\n",
    "Xminmax = MinMaxScaler().fit_transform(X)\n",
    "Xquantile1 = QuantileTransformer(output_distribution='uniform').fit_transform(X)\n",
    "Xquantile2 = QuantileTransformer(output_distribution='normal').fit_transform(X)\n",
    "Xpower1 = PowerTransformer(method='yeo-johnson').fit_transform(X)\n",
    "Xpower2 = PowerTransformer(method='box-cox').fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print type(X),type(Xstandard),type(Xrobust),type(Xnormalize),type(Xminmax),type(Xquantile1),type(Xquantile2),type(Xpower1),type(Xpower2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print X.shape, Xstandard.shape, Xrobust.shape, Xnormalize.shape, Xminmax.shape, Xquantile1.shape, Xquantile2.shape, Xpower1.shape, Xpower2.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Get labels\n",
    "y = labeledNYT['label_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transform the text class labels into numerical labels (multiclass classification)\n",
    "le = LabelEncoder()\n",
    "y_numeric = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print type(y_numeric), len(y_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transform the text class labels into numerical labels (multilabel classification)\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_indicator = mlb.fit_transform(y[:,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print type(y_indicator), y_indicator.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# le.classes_\n",
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inputs for SVM classifications:\n",
    "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovo', degree=3, gamma='scale',\n",
    "    kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False)\n",
    "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True, intercept_scaling=1, loss='squared_hinge',\n",
    "          max_iter=1000, multi_class='ovr', penalty='l2', random_state=None, tol=0.0001, verbose=0)\n",
    "C                           # 1 by default. Penalty parameter of the error term. Decrease if having many noisy observations. Trades off misclassification of training examples against simplicity of the decision surface. Low C makes decision surface smooth. High C aims at classifying all training examples correctly \n",
    "cache_size                  # Specify the size of the kernel cache (in MB). 200(MB) by default. Size of kernel cache has strong impact on run times for larger problems. If having enough RAM, set cache_size higher, such as 500(MB) or 1000(MB) \n",
    "class_weight                # Give more importance to certain classes. If data for classification are unbalanced (e.g. many positive and few negative), set class_weight='balanced' and/or try different penalty parameters C \n",
    "coef0                       # Independent term in kernel function for poly and sigmoid kernels \n",
    "decision_function_shape     # {'ovr','ovo'} OneVsRest and OneVsOne. Allows to aggregate results of “one-against-one” classifiers to a decision function of shape (n_samples, n_classes) to provide a consistent interface with other classifiers \n",
    "degree                      # Parameter for polynomial kernel \n",
    "gamma                       # {'auto','auto_deprecated','scale',float} for rbf, poly, and sigmoid kernels. Defines how much influence a single training example has (=Inverse of radius of influence of samples selected by the model as support vectors). The larger gamma is, the closer other examples must be to be affected. Should not be too large or too small \n",
    "kernel                      # {'linear','rbf','poly','sigmoid','precomputed'}\n",
    "max_iter                    # {-1,some natural number} \n",
    "probability                 # Whether to enable probability estimates. This must be enabled prior to calling fit, and will slow down that method. Use a random number generator to shuffle the data for probability estimation (=True). Estimators are not random (=False) \n",
    "random_state                # {int,RandomState instance,None}. Control the randomness when probability=True. No effect when probability=False \n",
    "shrinking                   # Whether to use the shrinking heuristic \n",
    "tol                         # Tolerance for stopping criterion. Try smaller tolerance to get same results for the same input data when set dual=True under LinearSVC \n",
    "verbose                     # {0,False}\n",
    "dual                        # Use random number generator to select features when fitting the model with a dual coordinate descent (=True). This randomness can also be controlled with the random_state parameter \n",
    "fit_intercept               # \n",
    "intercept_scaling           # \n",
    "loss                        # \n",
    "multi_class                 # \n",
    "penalty                     # \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "y_numeric is faster, LinearSVC>SVC(kernel='linear')>SVC(kernel='rbf'), cache_size=10000 is much faster.\n",
    "'rbf' and y_numeric increase accuracy.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Check if out-of-sample data has the same topics as in-sample data\n",
    "print len(set(y_numeric[:10000])), len(set(y_numeric[:11000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Set n_jobs=-1 to use all processors, n_jobs=None to use 1 processor, n_jobs=int to use some processors\n",
    "InSample1 = [X[:10000],Xstandard[:10000],Xrobust[:10000],Xnormalize[:10000],Xminmax[:10000],Xquantile1[:10000],\n",
    "           Xquantile2[:10000],Xpower1[:10000],Xpower2[:10000]]\n",
    "OutSample1 = [X[10000:11000],Xstandard[10000:11000],Xrobust[10000:11000],Xnormalize[10000:11000],Xminmax[10000:11000],\n",
    "           Xquantile1[10000:11000],Xquantile2[10000:11000],Xpower1[10000:11000],Xpower2[10000:11000]]\n",
    "InLabels1 = y_numeric[:10000]\n",
    "OutLabels1 = y_numeric[10000:11000]\n",
    "clf1 = OneVsRestClassifier(svm.LinearSVC(C=1,dual=False,max_iter=10000),n_jobs=-1)\n",
    "clf2 = OneVsRestClassifier(svm.LinearSVC(C=10,dual=False,max_iter=10000),n_jobs=-1)\n",
    "clf3 = OneVsRestClassifier(svm.SVC(C=1,cache_size=10000,gamma='scale',max_iter=-1,tol=0.0001),n_jobs=-1)\n",
    "clf4 = OneVsRestClassifier(svm.SVC(C=10,cache_size=10000,gamma='scale',max_iter=-1,tol=0.0001),n_jobs=-1)\n",
    "clfs1 = [clf1,clf2,clf3,clf4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count1 = 0\n",
    "Models1 = []\n",
    "InSampleAccuracy1 = []\n",
    "OutSampleAccuracy1 = []\n",
    "runtime1 = []\n",
    "for clf in clfs1:\n",
    "    for indx,sample in enumerate(InSample1):\n",
    "        count1 += 1\n",
    "        start_time = time.time()\n",
    "        model = clf.fit(sample,InLabels1)\n",
    "        Models1.append(model)\n",
    "        accuracy1 = clf.score(sample,InLabels1)\n",
    "        InSampleAccuracy1.append(accuracy1)\n",
    "        accuracy2 = clf.score(OutSample1[indx],OutLabels1)\n",
    "        OutSampleAccuracy1.append(accuracy2)\n",
    "        print 'Model ',count1,': ',model\n",
    "        print 'In-sample accuracy = ',accuracy1\n",
    "        print 'Out-of-sample accuracy = ',accuracy2\n",
    "        run_time = (time.time()-start_time)/60\n",
    "        runtime1.append(run_time)\n",
    "        print (\"This takes %s minutes to run\" %run_time)\n",
    "        print '--------------------------------------------------'\n",
    "### X,Xstandard,Xrobust,Xnormalize,Xminmax,Xquantile1,Xquantile2,Xpower1,Xpower2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Samplelist1 = ['X[:100000]','Xstandard[:100000]','Xrobust[:100000]','Xnormalize[:100000]','Xminmax[:100000]',\n",
    "               'Xquantile1[:100000]','Xquantile2[:100000]','Xpower1[:100000]','Xpower2[:100000]']*4\n",
    "Samplelist1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVCtests1 = pd.DataFrame([Models1,InSampleAccuracy1,OutSampleAccuracy1,runtime1,Samplelist1]).T\n",
    "SVCtests1.columns = ['Model','InSampleAccuracy','OutSampleAccuracy','RunTime','Sample']\n",
    "SVCtests1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Neural network, Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Get topics\n",
    "X = labeledNYT.iloc[:,20:120]\n",
    "# X = labeledNYT.iloc[:,120:220]\n",
    "X = X.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(X[1]), len(X[2]), X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Get labels\n",
    "y = labeledNYT['label_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_numeric = le.fit_transform(y)\n",
    "print type(y_numeric), len(y_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(set(y_numeric[:14000])), len(set(y_numeric[:28000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Convert data to tensors before feeding into the neural network\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(X[:14000],\n",
    "                                                        value=word_index[\"<PAD>\"],\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=100)\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(X[14000:28000],\n",
    "                                                       value=word_index[\"<PAD>\"],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Create a validation set from the train data\n",
    "x_val = X[10000:14000]\n",
    "partial_x_train = X[:10000]\n",
    "\n",
    "y_val = y_numeric[10000:14000]\n",
    "partial_y_train = y_numeric[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Input shape is the vocabulary count used for the movie reviews (12,9638 words)\n",
    "vocab_size = 129638\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size,16))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16,activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(1,activation=tf.nn.sigmoid))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Compile model\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(),loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Fit the model\n",
    "history = model.fit(partial_x_train,partial_y_train,epochs=40,batch_size=512,validation_data=(x_val,y_val),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Evaluate the model\n",
    "results = model.evaluate(X[14000:28000],y_numeric[14000:28000])\n",
    "print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()     # Clear figure\n",
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
